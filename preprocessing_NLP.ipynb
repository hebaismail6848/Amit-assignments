{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hebaismail6848/Amit-assignments/blob/main/preprocessing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhFIBdS2bDl4"
      },
      "source": [
        "#Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4QUwDmOaqaD",
        "outputId": "dd2281bb-bc2b-40b7-abc4-bf4ac88d2881"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk      # Natural language toolkit\n",
        "# nltk.download()  # download the necessary dataset, puckages, libraries(load memories)\n",
        "nltk.download('punkt')  # download the punkt resource,This resource is required for tokenization, including sentence tokenization and word tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYk8MvFQbrS7",
        "outputId": "e5859906-41b9-4b75-ca9c-972ea090a409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello!', 'How are you doing today?', \"I hope you're having a good day.\"]\n",
            "['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', 'hope', 'you', \"'re\", 'having', 'a', 'good', 'day', '.']\n"
          ]
        }
      ],
      "source": [
        "# tokenize the text into individual sentences\n",
        "text = \"Hello! How are you doing today? I hope you're having a good day.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(sentences)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtJ2nVPqluq2"
      },
      "source": [
        "#Stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjP5zNBKlw0a",
        "outputId": "28c8b92a-b457-41b3-b7b6-7c14325605ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlNxwU4emHQs"
      },
      "source": [
        "##stop words in Arabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jEd5hW1mIbU",
        "outputId": "1610c2fb-6abd-4ed0-d4a9-dfc06d2b41fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['إذ',\n",
              " 'إذا',\n",
              " 'إذما',\n",
              " 'إذن',\n",
              " 'أف',\n",
              " 'أقل',\n",
              " 'أكثر',\n",
              " 'ألا',\n",
              " 'إلا',\n",
              " 'التي',\n",
              " 'الذي',\n",
              " 'الذين',\n",
              " 'اللاتي',\n",
              " 'اللائي',\n",
              " 'اللتان',\n",
              " 'اللتيا',\n",
              " 'اللتين',\n",
              " 'اللذان',\n",
              " 'اللذين',\n",
              " 'اللواتي',\n",
              " 'إلى',\n",
              " 'إليك',\n",
              " 'إليكم',\n",
              " 'إليكما',\n",
              " 'إليكن',\n",
              " 'أم',\n",
              " 'أما',\n",
              " 'أما',\n",
              " 'إما',\n",
              " 'أن',\n",
              " 'إن',\n",
              " 'إنا',\n",
              " 'أنا',\n",
              " 'أنت',\n",
              " 'أنتم',\n",
              " 'أنتما',\n",
              " 'أنتن',\n",
              " 'إنما',\n",
              " 'إنه',\n",
              " 'أنى',\n",
              " 'أنى',\n",
              " 'آه',\n",
              " 'آها',\n",
              " 'أو',\n",
              " 'أولاء',\n",
              " 'أولئك',\n",
              " 'أوه',\n",
              " 'آي',\n",
              " 'أي',\n",
              " 'أيها',\n",
              " 'إي',\n",
              " 'أين',\n",
              " 'أين',\n",
              " 'أينما',\n",
              " 'إيه',\n",
              " 'بخ',\n",
              " 'بس',\n",
              " 'بعد',\n",
              " 'بعض',\n",
              " 'بك',\n",
              " 'بكم',\n",
              " 'بكم',\n",
              " 'بكما',\n",
              " 'بكن',\n",
              " 'بل',\n",
              " 'بلى',\n",
              " 'بما',\n",
              " 'بماذا',\n",
              " 'بمن',\n",
              " 'بنا',\n",
              " 'به',\n",
              " 'بها',\n",
              " 'بهم',\n",
              " 'بهما',\n",
              " 'بهن',\n",
              " 'بي',\n",
              " 'بين',\n",
              " 'بيد',\n",
              " 'تلك',\n",
              " 'تلكم',\n",
              " 'تلكما',\n",
              " 'ته',\n",
              " 'تي',\n",
              " 'تين',\n",
              " 'تينك',\n",
              " 'ثم',\n",
              " 'ثمة',\n",
              " 'حاشا',\n",
              " 'حبذا',\n",
              " 'حتى',\n",
              " 'حيث',\n",
              " 'حيثما',\n",
              " 'حين',\n",
              " 'خلا',\n",
              " 'دون',\n",
              " 'ذا',\n",
              " 'ذات',\n",
              " 'ذاك',\n",
              " 'ذان',\n",
              " 'ذانك',\n",
              " 'ذلك',\n",
              " 'ذلكم',\n",
              " 'ذلكما',\n",
              " 'ذلكن',\n",
              " 'ذه',\n",
              " 'ذو',\n",
              " 'ذوا',\n",
              " 'ذواتا',\n",
              " 'ذواتي',\n",
              " 'ذي',\n",
              " 'ذين',\n",
              " 'ذينك',\n",
              " 'ريث',\n",
              " 'سوف',\n",
              " 'سوى',\n",
              " 'شتان',\n",
              " 'عدا',\n",
              " 'عسى',\n",
              " 'عل',\n",
              " 'على',\n",
              " 'عليك',\n",
              " 'عليه',\n",
              " 'عما',\n",
              " 'عن',\n",
              " 'عند',\n",
              " 'غير',\n",
              " 'فإذا',\n",
              " 'فإن',\n",
              " 'فلا',\n",
              " 'فمن',\n",
              " 'في',\n",
              " 'فيم',\n",
              " 'فيما',\n",
              " 'فيه',\n",
              " 'فيها',\n",
              " 'قد',\n",
              " 'كأن',\n",
              " 'كأنما',\n",
              " 'كأي',\n",
              " 'كأين',\n",
              " 'كذا',\n",
              " 'كذلك',\n",
              " 'كل',\n",
              " 'كلا',\n",
              " 'كلاهما',\n",
              " 'كلتا',\n",
              " 'كلما',\n",
              " 'كليكما',\n",
              " 'كليهما',\n",
              " 'كم',\n",
              " 'كم',\n",
              " 'كما',\n",
              " 'كي',\n",
              " 'كيت',\n",
              " 'كيف',\n",
              " 'كيفما',\n",
              " 'لا',\n",
              " 'لاسيما',\n",
              " 'لدى',\n",
              " 'لست',\n",
              " 'لستم',\n",
              " 'لستما',\n",
              " 'لستن',\n",
              " 'لسن',\n",
              " 'لسنا',\n",
              " 'لعل',\n",
              " 'لك',\n",
              " 'لكم',\n",
              " 'لكما',\n",
              " 'لكن',\n",
              " 'لكنما',\n",
              " 'لكي',\n",
              " 'لكيلا',\n",
              " 'لم',\n",
              " 'لما',\n",
              " 'لن',\n",
              " 'لنا',\n",
              " 'له',\n",
              " 'لها',\n",
              " 'لهم',\n",
              " 'لهما',\n",
              " 'لهن',\n",
              " 'لو',\n",
              " 'لولا',\n",
              " 'لوما',\n",
              " 'لي',\n",
              " 'لئن',\n",
              " 'ليت',\n",
              " 'ليس',\n",
              " 'ليسا',\n",
              " 'ليست',\n",
              " 'ليستا',\n",
              " 'ليسوا',\n",
              " 'ما',\n",
              " 'ماذا',\n",
              " 'متى',\n",
              " 'مذ',\n",
              " 'مع',\n",
              " 'مما',\n",
              " 'ممن',\n",
              " 'من',\n",
              " 'منه',\n",
              " 'منها',\n",
              " 'منذ',\n",
              " 'مه',\n",
              " 'مهما',\n",
              " 'نحن',\n",
              " 'نحو',\n",
              " 'نعم',\n",
              " 'ها',\n",
              " 'هاتان',\n",
              " 'هاته',\n",
              " 'هاتي',\n",
              " 'هاتين',\n",
              " 'هاك',\n",
              " 'هاهنا',\n",
              " 'هذا',\n",
              " 'هذان',\n",
              " 'هذه',\n",
              " 'هذي',\n",
              " 'هذين',\n",
              " 'هكذا',\n",
              " 'هل',\n",
              " 'هلا',\n",
              " 'هم',\n",
              " 'هما',\n",
              " 'هن',\n",
              " 'هنا',\n",
              " 'هناك',\n",
              " 'هنالك',\n",
              " 'هو',\n",
              " 'هؤلاء',\n",
              " 'هي',\n",
              " 'هيا',\n",
              " 'هيت',\n",
              " 'هيهات',\n",
              " 'والذي',\n",
              " 'والذين',\n",
              " 'وإذ',\n",
              " 'وإذا',\n",
              " 'وإن',\n",
              " 'ولا',\n",
              " 'ولكن',\n",
              " 'ولو',\n",
              " 'وما',\n",
              " 'ومن',\n",
              " 'وهو',\n",
              " 'يا',\n",
              " 'أبٌ',\n",
              " 'أخٌ',\n",
              " 'حمٌ',\n",
              " 'فو',\n",
              " 'أنتِ',\n",
              " 'يناير',\n",
              " 'فبراير',\n",
              " 'مارس',\n",
              " 'أبريل',\n",
              " 'مايو',\n",
              " 'يونيو',\n",
              " 'يوليو',\n",
              " 'أغسطس',\n",
              " 'سبتمبر',\n",
              " 'أكتوبر',\n",
              " 'نوفمبر',\n",
              " 'ديسمبر',\n",
              " 'جانفي',\n",
              " 'فيفري',\n",
              " 'مارس',\n",
              " 'أفريل',\n",
              " 'ماي',\n",
              " 'جوان',\n",
              " 'جويلية',\n",
              " 'أوت',\n",
              " 'كانون',\n",
              " 'شباط',\n",
              " 'آذار',\n",
              " 'نيسان',\n",
              " 'أيار',\n",
              " 'حزيران',\n",
              " 'تموز',\n",
              " 'آب',\n",
              " 'أيلول',\n",
              " 'تشرين',\n",
              " 'دولار',\n",
              " 'دينار',\n",
              " 'ريال',\n",
              " 'درهم',\n",
              " 'ليرة',\n",
              " 'جنيه',\n",
              " 'قرش',\n",
              " 'مليم',\n",
              " 'فلس',\n",
              " 'هللة',\n",
              " 'سنتيم',\n",
              " 'يورو',\n",
              " 'ين',\n",
              " 'يوان',\n",
              " 'شيكل',\n",
              " 'واحد',\n",
              " 'اثنان',\n",
              " 'ثلاثة',\n",
              " 'أربعة',\n",
              " 'خمسة',\n",
              " 'ستة',\n",
              " 'سبعة',\n",
              " 'ثمانية',\n",
              " 'تسعة',\n",
              " 'عشرة',\n",
              " 'أحد',\n",
              " 'اثنا',\n",
              " 'اثني',\n",
              " 'إحدى',\n",
              " 'ثلاث',\n",
              " 'أربع',\n",
              " 'خمس',\n",
              " 'ست',\n",
              " 'سبع',\n",
              " 'ثماني',\n",
              " 'تسع',\n",
              " 'عشر',\n",
              " 'ثمان',\n",
              " 'سبت',\n",
              " 'أحد',\n",
              " 'اثنين',\n",
              " 'ثلاثاء',\n",
              " 'أربعاء',\n",
              " 'خميس',\n",
              " 'جمعة',\n",
              " 'أول',\n",
              " 'ثان',\n",
              " 'ثاني',\n",
              " 'ثالث',\n",
              " 'رابع',\n",
              " 'خامس',\n",
              " 'سادس',\n",
              " 'سابع',\n",
              " 'ثامن',\n",
              " 'تاسع',\n",
              " 'عاشر',\n",
              " 'حادي',\n",
              " 'أ',\n",
              " 'ب',\n",
              " 'ت',\n",
              " 'ث',\n",
              " 'ج',\n",
              " 'ح',\n",
              " 'خ',\n",
              " 'د',\n",
              " 'ذ',\n",
              " 'ر',\n",
              " 'ز',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ص',\n",
              " 'ض',\n",
              " 'ط',\n",
              " 'ظ',\n",
              " 'ع',\n",
              " 'غ',\n",
              " 'ف',\n",
              " 'ق',\n",
              " 'ك',\n",
              " 'ل',\n",
              " 'م',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ي',\n",
              " 'ء',\n",
              " 'ى',\n",
              " 'آ',\n",
              " 'ؤ',\n",
              " 'ئ',\n",
              " 'أ',\n",
              " 'ة',\n",
              " 'ألف',\n",
              " 'باء',\n",
              " 'تاء',\n",
              " 'ثاء',\n",
              " 'جيم',\n",
              " 'حاء',\n",
              " 'خاء',\n",
              " 'دال',\n",
              " 'ذال',\n",
              " 'راء',\n",
              " 'زاي',\n",
              " 'سين',\n",
              " 'شين',\n",
              " 'صاد',\n",
              " 'ضاد',\n",
              " 'طاء',\n",
              " 'ظاء',\n",
              " 'عين',\n",
              " 'غين',\n",
              " 'فاء',\n",
              " 'قاف',\n",
              " 'كاف',\n",
              " 'لام',\n",
              " 'ميم',\n",
              " 'نون',\n",
              " 'هاء',\n",
              " 'واو',\n",
              " 'ياء',\n",
              " 'همزة',\n",
              " 'ي',\n",
              " 'نا',\n",
              " 'ك',\n",
              " 'كن',\n",
              " 'ه',\n",
              " 'إياه',\n",
              " 'إياها',\n",
              " 'إياهما',\n",
              " 'إياهم',\n",
              " 'إياهن',\n",
              " 'إياك',\n",
              " 'إياكما',\n",
              " 'إياكم',\n",
              " 'إياك',\n",
              " 'إياكن',\n",
              " 'إياي',\n",
              " 'إيانا',\n",
              " 'أولالك',\n",
              " 'تانِ',\n",
              " 'تانِك',\n",
              " 'تِه',\n",
              " 'تِي',\n",
              " 'تَيْنِ',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'ذانِ',\n",
              " 'ذِه',\n",
              " 'ذِي',\n",
              " 'ذَيْنِ',\n",
              " 'هَؤلاء',\n",
              " 'هَاتانِ',\n",
              " 'هَاتِه',\n",
              " 'هَاتِي',\n",
              " 'هَاتَيْنِ',\n",
              " 'هَذا',\n",
              " 'هَذانِ',\n",
              " 'هَذِه',\n",
              " 'هَذِي',\n",
              " 'هَذَيْنِ',\n",
              " 'الألى',\n",
              " 'الألاء',\n",
              " 'أل',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'ذيت',\n",
              " 'كأيّ',\n",
              " 'كأيّن',\n",
              " 'بضع',\n",
              " 'فلان',\n",
              " 'وا',\n",
              " 'آمينَ',\n",
              " 'آهِ',\n",
              " 'آهٍ',\n",
              " 'آهاً',\n",
              " 'أُفٍّ',\n",
              " 'أُفٍّ',\n",
              " 'أفٍّ',\n",
              " 'أمامك',\n",
              " 'أمامكَ',\n",
              " 'أوّهْ',\n",
              " 'إلَيْكَ',\n",
              " 'إلَيْكَ',\n",
              " 'إليكَ',\n",
              " 'إليكنّ',\n",
              " 'إيهٍ',\n",
              " 'بخٍ',\n",
              " 'بسّ',\n",
              " 'بَسْ',\n",
              " 'بطآن',\n",
              " 'بَلْهَ',\n",
              " 'حاي',\n",
              " 'حَذارِ',\n",
              " 'حيَّ',\n",
              " 'حيَّ',\n",
              " 'دونك',\n",
              " 'رويدك',\n",
              " 'سرعان',\n",
              " 'شتانَ',\n",
              " 'شَتَّانَ',\n",
              " 'صهْ',\n",
              " 'صهٍ',\n",
              " 'طاق',\n",
              " 'طَق',\n",
              " 'عَدَسْ',\n",
              " 'كِخ',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانكم',\n",
              " 'مكانكما',\n",
              " 'مكانكنّ',\n",
              " 'نَخْ',\n",
              " 'هاكَ',\n",
              " 'هَجْ',\n",
              " 'هلم',\n",
              " 'هيّا',\n",
              " 'هَيْهات',\n",
              " 'وا',\n",
              " 'واهاً',\n",
              " 'وراءَك',\n",
              " 'وُشْكَانَ',\n",
              " 'وَيْ',\n",
              " 'يفعلان',\n",
              " 'تفعلان',\n",
              " 'يفعلون',\n",
              " 'تفعلون',\n",
              " 'تفعلين',\n",
              " 'اتخذ',\n",
              " 'ألفى',\n",
              " 'تخذ',\n",
              " 'ترك',\n",
              " 'تعلَّم',\n",
              " 'جعل',\n",
              " 'حجا',\n",
              " 'حبيب',\n",
              " 'خال',\n",
              " 'حسب',\n",
              " 'خال',\n",
              " 'درى',\n",
              " 'رأى',\n",
              " 'زعم',\n",
              " 'صبر',\n",
              " 'ظنَّ',\n",
              " 'عدَّ',\n",
              " 'علم',\n",
              " 'غادر',\n",
              " 'ذهب',\n",
              " 'وجد',\n",
              " 'ورد',\n",
              " 'وهب',\n",
              " 'أسكن',\n",
              " 'أطعم',\n",
              " 'أعطى',\n",
              " 'رزق',\n",
              " 'زود',\n",
              " 'سقى',\n",
              " 'كسا',\n",
              " 'أخبر',\n",
              " 'أرى',\n",
              " 'أعلم',\n",
              " 'أنبأ',\n",
              " 'حدَث',\n",
              " 'خبَّر',\n",
              " 'نبَّا',\n",
              " 'أفعل به',\n",
              " 'ما أفعله',\n",
              " 'بئس',\n",
              " 'ساء',\n",
              " 'طالما',\n",
              " 'قلما',\n",
              " 'لات',\n",
              " 'لكنَّ',\n",
              " 'ءَ',\n",
              " 'أجل',\n",
              " 'إذاً',\n",
              " 'أمّا',\n",
              " 'إمّا',\n",
              " 'إنَّ',\n",
              " 'أنًّ',\n",
              " 'أى',\n",
              " 'إى',\n",
              " 'أيا',\n",
              " 'ب',\n",
              " 'ثمَّ',\n",
              " 'جلل',\n",
              " 'جير',\n",
              " 'رُبَّ',\n",
              " 'س',\n",
              " 'علًّ',\n",
              " 'ف',\n",
              " 'كأنّ',\n",
              " 'كلَّا',\n",
              " 'كى',\n",
              " 'ل',\n",
              " 'لات',\n",
              " 'لعلَّ',\n",
              " 'لكنَّ',\n",
              " 'لكنَّ',\n",
              " 'م',\n",
              " 'نَّ',\n",
              " 'هلّا',\n",
              " 'وا',\n",
              " 'أل',\n",
              " 'إلّا',\n",
              " 'ت',\n",
              " 'ك',\n",
              " 'لمّا',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ا',\n",
              " 'ي',\n",
              " 'تجاه',\n",
              " 'تلقاء',\n",
              " 'جميع',\n",
              " 'حسب',\n",
              " 'سبحان',\n",
              " 'شبه',\n",
              " 'لعمر',\n",
              " 'مثل',\n",
              " 'معاذ',\n",
              " 'أبو',\n",
              " 'أخو',\n",
              " 'حمو',\n",
              " 'فو',\n",
              " 'مئة',\n",
              " 'مئتان',\n",
              " 'ثلاثمئة',\n",
              " 'أربعمئة',\n",
              " 'خمسمئة',\n",
              " 'ستمئة',\n",
              " 'سبعمئة',\n",
              " 'ثمنمئة',\n",
              " 'تسعمئة',\n",
              " 'مائة',\n",
              " 'ثلاثمائة',\n",
              " 'أربعمائة',\n",
              " 'خمسمائة',\n",
              " 'ستمائة',\n",
              " 'سبعمائة',\n",
              " 'ثمانمئة',\n",
              " 'تسعمائة',\n",
              " 'عشرون',\n",
              " 'ثلاثون',\n",
              " 'اربعون',\n",
              " 'خمسون',\n",
              " 'ستون',\n",
              " 'سبعون',\n",
              " 'ثمانون',\n",
              " 'تسعون',\n",
              " 'عشرين',\n",
              " 'ثلاثين',\n",
              " 'اربعين',\n",
              " 'خمسين',\n",
              " 'ستين',\n",
              " 'سبعين',\n",
              " 'ثمانين',\n",
              " 'تسعين',\n",
              " 'بضع',\n",
              " 'نيف',\n",
              " 'أجمع',\n",
              " 'جميع',\n",
              " 'عامة',\n",
              " 'عين',\n",
              " 'نفس',\n",
              " 'لا سيما',\n",
              " 'أصلا',\n",
              " 'أهلا',\n",
              " 'أيضا',\n",
              " 'بؤسا',\n",
              " 'بعدا',\n",
              " 'بغتة',\n",
              " 'تعسا',\n",
              " 'حقا',\n",
              " 'حمدا',\n",
              " 'خلافا',\n",
              " 'خاصة',\n",
              " 'دواليك',\n",
              " 'سحقا',\n",
              " 'سرا',\n",
              " 'سمعا',\n",
              " 'صبرا',\n",
              " 'صدقا',\n",
              " 'صراحة',\n",
              " 'طرا',\n",
              " 'عجبا',\n",
              " 'عيانا',\n",
              " 'غالبا',\n",
              " 'فرادى',\n",
              " 'فضلا',\n",
              " 'قاطبة',\n",
              " 'كثيرا',\n",
              " 'لبيك',\n",
              " 'معاذ',\n",
              " 'أبدا',\n",
              " 'إزاء',\n",
              " 'أصلا',\n",
              " 'الآن',\n",
              " 'أمد',\n",
              " 'أمس',\n",
              " 'آنفا',\n",
              " 'آناء',\n",
              " 'أنّى',\n",
              " 'أول',\n",
              " 'أيّان',\n",
              " 'تارة',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'حقا',\n",
              " 'صباح',\n",
              " 'مساء',\n",
              " 'ضحوة',\n",
              " 'عوض',\n",
              " 'غدا',\n",
              " 'غداة',\n",
              " 'قطّ',\n",
              " 'كلّما',\n",
              " 'لدن',\n",
              " 'لمّا',\n",
              " 'مرّة',\n",
              " 'قبل',\n",
              " 'خلف',\n",
              " 'أمام',\n",
              " 'فوق',\n",
              " 'تحت',\n",
              " 'يمين',\n",
              " 'شمال',\n",
              " 'ارتدّ',\n",
              " 'استحال',\n",
              " 'أصبح',\n",
              " 'أضحى',\n",
              " 'آض',\n",
              " 'أمسى',\n",
              " 'انقلب',\n",
              " 'بات',\n",
              " 'تبدّل',\n",
              " 'تحوّل',\n",
              " 'حار',\n",
              " 'رجع',\n",
              " 'راح',\n",
              " 'صار',\n",
              " 'ظلّ',\n",
              " 'عاد',\n",
              " 'غدا',\n",
              " 'كان',\n",
              " 'ما انفك',\n",
              " 'ما برح',\n",
              " 'مادام',\n",
              " 'مازال',\n",
              " 'مافتئ',\n",
              " 'ابتدأ',\n",
              " 'أخذ',\n",
              " 'اخلولق',\n",
              " 'أقبل',\n",
              " 'انبرى',\n",
              " 'أنشأ',\n",
              " 'أوشك',\n",
              " 'جعل',\n",
              " 'حرى',\n",
              " 'شرع',\n",
              " 'طفق',\n",
              " 'علق',\n",
              " 'قام',\n",
              " 'كرب',\n",
              " 'كاد',\n",
              " 'هبّ']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('arabic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUvUmHO7gW59",
        "outputId": "4d605a4e-7c10-4d56-ff85-6623ee7dd97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n",
            "['قامت', 'الشركة', 'بإصدار', 'بيانات', 'تفصيلية', 'حول', 'أدائها', 'في', 'الربع', 'الأول', 'من', 'العام', 'الحالي']\n"
          ]
        }
      ],
      "source": [
        "# Define a sample Arabic text\n",
        "arabic_text =\"قامت الشركة بإصدار بيانات تفصيلية حول أدائها في الربع الأول من العام الحالي\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(arabic_text)\n",
        "print(len(words))\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBVeExxgmTn4",
        "outputId": "34925a51-7820-4778-c227-e8ed5b5229c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ء',\n",
              " 'ءَ',\n",
              " 'آ',\n",
              " 'آب',\n",
              " 'آذار',\n",
              " 'آض',\n",
              " 'آمينَ',\n",
              " 'آناء',\n",
              " 'آنفا',\n",
              " 'آه',\n",
              " 'آها',\n",
              " 'آهاً',\n",
              " 'آهٍ',\n",
              " 'آهِ',\n",
              " 'آي',\n",
              " 'أ',\n",
              " 'أبدا',\n",
              " 'أبريل',\n",
              " 'أبو',\n",
              " 'أبٌ',\n",
              " 'أجل',\n",
              " 'أجمع',\n",
              " 'أحد',\n",
              " 'أخبر',\n",
              " 'أخذ',\n",
              " 'أخو',\n",
              " 'أخٌ',\n",
              " 'أربع',\n",
              " 'أربعاء',\n",
              " 'أربعة',\n",
              " 'أربعمئة',\n",
              " 'أربعمائة',\n",
              " 'أرى',\n",
              " 'أسكن',\n",
              " 'أصبح',\n",
              " 'أصلا',\n",
              " 'أضحى',\n",
              " 'أطعم',\n",
              " 'أعطى',\n",
              " 'أعلم',\n",
              " 'أغسطس',\n",
              " 'أف',\n",
              " 'أفريل',\n",
              " 'أفعل به',\n",
              " 'أفٍّ',\n",
              " 'أقبل',\n",
              " 'أقل',\n",
              " 'أكتوبر',\n",
              " 'أكثر',\n",
              " 'أل',\n",
              " 'ألا',\n",
              " 'ألف',\n",
              " 'ألفى',\n",
              " 'أم',\n",
              " 'أما',\n",
              " 'أمام',\n",
              " 'أمامك',\n",
              " 'أمامكَ',\n",
              " 'أمد',\n",
              " 'أمس',\n",
              " 'أمسى',\n",
              " 'أمّا',\n",
              " 'أن',\n",
              " 'أنا',\n",
              " 'أنبأ',\n",
              " 'أنت',\n",
              " 'أنتم',\n",
              " 'أنتما',\n",
              " 'أنتن',\n",
              " 'أنتِ',\n",
              " 'أنشأ',\n",
              " 'أنى',\n",
              " 'أنًّ',\n",
              " 'أنّى',\n",
              " 'أهلا',\n",
              " 'أو',\n",
              " 'أوت',\n",
              " 'أوشك',\n",
              " 'أول',\n",
              " 'أولئك',\n",
              " 'أولاء',\n",
              " 'أولالك',\n",
              " 'أوه',\n",
              " 'أوّهْ',\n",
              " 'أى',\n",
              " 'أي',\n",
              " 'أيا',\n",
              " 'أيار',\n",
              " 'أيضا',\n",
              " 'أيلول',\n",
              " 'أين',\n",
              " 'أينما',\n",
              " 'أيها',\n",
              " 'أيّ',\n",
              " 'أيّان',\n",
              " 'أُفٍّ',\n",
              " 'ؤ',\n",
              " 'إحدى',\n",
              " 'إذ',\n",
              " 'إذا',\n",
              " 'إذاً',\n",
              " 'إذما',\n",
              " 'إذن',\n",
              " 'إزاء',\n",
              " 'إلا',\n",
              " 'إلى',\n",
              " 'إليك',\n",
              " 'إليكم',\n",
              " 'إليكما',\n",
              " 'إليكن',\n",
              " 'إليكنّ',\n",
              " 'إليكَ',\n",
              " 'إلَيْكَ',\n",
              " 'إلّا',\n",
              " 'إما',\n",
              " 'إمّا',\n",
              " 'إن',\n",
              " 'إنا',\n",
              " 'إنما',\n",
              " 'إنه',\n",
              " 'إنَّ',\n",
              " 'إى',\n",
              " 'إي',\n",
              " 'إياك',\n",
              " 'إياكم',\n",
              " 'إياكما',\n",
              " 'إياكن',\n",
              " 'إيانا',\n",
              " 'إياه',\n",
              " 'إياها',\n",
              " 'إياهم',\n",
              " 'إياهما',\n",
              " 'إياهن',\n",
              " 'إياي',\n",
              " 'إيه',\n",
              " 'إيهٍ',\n",
              " 'ئ',\n",
              " 'ا',\n",
              " 'ابتدأ',\n",
              " 'اتخذ',\n",
              " 'اثنا',\n",
              " 'اثنان',\n",
              " 'اثني',\n",
              " 'اثنين',\n",
              " 'اخلولق',\n",
              " 'اربعون',\n",
              " 'اربعين',\n",
              " 'ارتدّ',\n",
              " 'استحال',\n",
              " 'الآن',\n",
              " 'الألاء',\n",
              " 'الألى',\n",
              " 'التي',\n",
              " 'الذي',\n",
              " 'الذين',\n",
              " 'اللائي',\n",
              " 'اللاتي',\n",
              " 'اللتان',\n",
              " 'اللتيا',\n",
              " 'اللتين',\n",
              " 'اللذان',\n",
              " 'اللذين',\n",
              " 'اللواتي',\n",
              " 'انبرى',\n",
              " 'انقلب',\n",
              " 'ب',\n",
              " 'بؤسا',\n",
              " 'بئس',\n",
              " 'باء',\n",
              " 'بات',\n",
              " 'بخ',\n",
              " 'بخٍ',\n",
              " 'بس',\n",
              " 'بسّ',\n",
              " 'بضع',\n",
              " 'بطآن',\n",
              " 'بعد',\n",
              " 'بعدا',\n",
              " 'بعض',\n",
              " 'بغتة',\n",
              " 'بك',\n",
              " 'بكم',\n",
              " 'بكما',\n",
              " 'بكن',\n",
              " 'بل',\n",
              " 'بلى',\n",
              " 'بما',\n",
              " 'بماذا',\n",
              " 'بمن',\n",
              " 'بنا',\n",
              " 'به',\n",
              " 'بها',\n",
              " 'بهم',\n",
              " 'بهما',\n",
              " 'بهن',\n",
              " 'بي',\n",
              " 'بيد',\n",
              " 'بين',\n",
              " 'بَسْ',\n",
              " 'بَلْهَ',\n",
              " 'ة',\n",
              " 'ت',\n",
              " 'تاء',\n",
              " 'تارة',\n",
              " 'تاسع',\n",
              " 'تانِ',\n",
              " 'تانِك',\n",
              " 'تبدّل',\n",
              " 'تجاه',\n",
              " 'تحت',\n",
              " 'تحوّل',\n",
              " 'تخذ',\n",
              " 'ترك',\n",
              " 'تسع',\n",
              " 'تسعة',\n",
              " 'تسعمئة',\n",
              " 'تسعمائة',\n",
              " 'تسعون',\n",
              " 'تسعين',\n",
              " 'تشرين',\n",
              " 'تعسا',\n",
              " 'تعلَّم',\n",
              " 'تفعلان',\n",
              " 'تفعلون',\n",
              " 'تفعلين',\n",
              " 'تلقاء',\n",
              " 'تلك',\n",
              " 'تلكم',\n",
              " 'تلكما',\n",
              " 'تموز',\n",
              " 'ته',\n",
              " 'تي',\n",
              " 'تين',\n",
              " 'تينك',\n",
              " 'تَيْنِ',\n",
              " 'تِه',\n",
              " 'تِي',\n",
              " 'ث',\n",
              " 'ثاء',\n",
              " 'ثالث',\n",
              " 'ثامن',\n",
              " 'ثان',\n",
              " 'ثاني',\n",
              " 'ثلاث',\n",
              " 'ثلاثاء',\n",
              " 'ثلاثة',\n",
              " 'ثلاثمئة',\n",
              " 'ثلاثمائة',\n",
              " 'ثلاثون',\n",
              " 'ثلاثين',\n",
              " 'ثم',\n",
              " 'ثمان',\n",
              " 'ثمانمئة',\n",
              " 'ثمانون',\n",
              " 'ثماني',\n",
              " 'ثمانية',\n",
              " 'ثمانين',\n",
              " 'ثمة',\n",
              " 'ثمنمئة',\n",
              " 'ثمَّ',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'ج',\n",
              " 'جانفي',\n",
              " 'جعل',\n",
              " 'جلل',\n",
              " 'جمعة',\n",
              " 'جميع',\n",
              " 'جنيه',\n",
              " 'جوان',\n",
              " 'جويلية',\n",
              " 'جير',\n",
              " 'جيم',\n",
              " 'ح',\n",
              " 'حاء',\n",
              " 'حادي',\n",
              " 'حار',\n",
              " 'حاشا',\n",
              " 'حاي',\n",
              " 'حبذا',\n",
              " 'حبيب',\n",
              " 'حتى',\n",
              " 'حجا',\n",
              " 'حدَث',\n",
              " 'حرى',\n",
              " 'حزيران',\n",
              " 'حسب',\n",
              " 'حقا',\n",
              " 'حمدا',\n",
              " 'حمو',\n",
              " 'حمٌ',\n",
              " 'حيث',\n",
              " 'حيثما',\n",
              " 'حين',\n",
              " 'حيَّ',\n",
              " 'حَذارِ',\n",
              " 'خ',\n",
              " 'خاء',\n",
              " 'خاصة',\n",
              " 'خال',\n",
              " 'خامس',\n",
              " 'خبَّر',\n",
              " 'خلا',\n",
              " 'خلافا',\n",
              " 'خلف',\n",
              " 'خمس',\n",
              " 'خمسة',\n",
              " 'خمسمئة',\n",
              " 'خمسمائة',\n",
              " 'خمسون',\n",
              " 'خمسين',\n",
              " 'خميس',\n",
              " 'د',\n",
              " 'دال',\n",
              " 'درهم',\n",
              " 'درى',\n",
              " 'دواليك',\n",
              " 'دولار',\n",
              " 'دون',\n",
              " 'دونك',\n",
              " 'ديسمبر',\n",
              " 'دينار',\n",
              " 'ذ',\n",
              " 'ذا',\n",
              " 'ذات',\n",
              " 'ذاك',\n",
              " 'ذال',\n",
              " 'ذان',\n",
              " 'ذانك',\n",
              " 'ذانِ',\n",
              " 'ذلك',\n",
              " 'ذلكم',\n",
              " 'ذلكما',\n",
              " 'ذلكن',\n",
              " 'ذه',\n",
              " 'ذهب',\n",
              " 'ذو',\n",
              " 'ذوا',\n",
              " 'ذواتا',\n",
              " 'ذواتي',\n",
              " 'ذي',\n",
              " 'ذيت',\n",
              " 'ذين',\n",
              " 'ذينك',\n",
              " 'ذَيْنِ',\n",
              " 'ذِه',\n",
              " 'ذِي',\n",
              " 'ر',\n",
              " 'رأى',\n",
              " 'راء',\n",
              " 'رابع',\n",
              " 'راح',\n",
              " 'رجع',\n",
              " 'رزق',\n",
              " 'رويدك',\n",
              " 'ريال',\n",
              " 'ريث',\n",
              " 'رُبَّ',\n",
              " 'ز',\n",
              " 'زاي',\n",
              " 'زعم',\n",
              " 'زود',\n",
              " 'س',\n",
              " 'ساء',\n",
              " 'سابع',\n",
              " 'سادس',\n",
              " 'سبت',\n",
              " 'سبتمبر',\n",
              " 'سبحان',\n",
              " 'سبع',\n",
              " 'سبعة',\n",
              " 'سبعمئة',\n",
              " 'سبعمائة',\n",
              " 'سبعون',\n",
              " 'سبعين',\n",
              " 'ست',\n",
              " 'ستة',\n",
              " 'ستمئة',\n",
              " 'ستمائة',\n",
              " 'ستون',\n",
              " 'ستين',\n",
              " 'سحقا',\n",
              " 'سرا',\n",
              " 'سرعان',\n",
              " 'سقى',\n",
              " 'سمعا',\n",
              " 'سنتيم',\n",
              " 'سوف',\n",
              " 'سوى',\n",
              " 'سين',\n",
              " 'ش',\n",
              " 'شباط',\n",
              " 'شبه',\n",
              " 'شتان',\n",
              " 'شتانَ',\n",
              " 'شرع',\n",
              " 'شمال',\n",
              " 'شيكل',\n",
              " 'شين',\n",
              " 'شَتَّانَ',\n",
              " 'ص',\n",
              " 'صاد',\n",
              " 'صار',\n",
              " 'صباح',\n",
              " 'صبر',\n",
              " 'صبرا',\n",
              " 'صدقا',\n",
              " 'صراحة',\n",
              " 'صهٍ',\n",
              " 'صهْ',\n",
              " 'ض',\n",
              " 'ضاد',\n",
              " 'ضحوة',\n",
              " 'ط',\n",
              " 'طاء',\n",
              " 'طاق',\n",
              " 'طالما',\n",
              " 'طرا',\n",
              " 'طفق',\n",
              " 'طَق',\n",
              " 'ظ',\n",
              " 'ظاء',\n",
              " 'ظلّ',\n",
              " 'ظنَّ',\n",
              " 'ع',\n",
              " 'عاد',\n",
              " 'عاشر',\n",
              " 'عامة',\n",
              " 'عجبا',\n",
              " 'عدا',\n",
              " 'عدَّ',\n",
              " 'عسى',\n",
              " 'عشر',\n",
              " 'عشرة',\n",
              " 'عشرون',\n",
              " 'عشرين',\n",
              " 'عل',\n",
              " 'علق',\n",
              " 'علم',\n",
              " 'على',\n",
              " 'عليك',\n",
              " 'عليه',\n",
              " 'علًّ',\n",
              " 'عما',\n",
              " 'عن',\n",
              " 'عند',\n",
              " 'عوض',\n",
              " 'عيانا',\n",
              " 'عين',\n",
              " 'عَدَسْ',\n",
              " 'غ',\n",
              " 'غادر',\n",
              " 'غالبا',\n",
              " 'غدا',\n",
              " 'غداة',\n",
              " 'غير',\n",
              " 'غين',\n",
              " 'ف',\n",
              " 'فإذا',\n",
              " 'فإن',\n",
              " 'فاء',\n",
              " 'فبراير',\n",
              " 'فرادى',\n",
              " 'فضلا',\n",
              " 'فلا',\n",
              " 'فلان',\n",
              " 'فلس',\n",
              " 'فمن',\n",
              " 'فو',\n",
              " 'فوق',\n",
              " 'في',\n",
              " 'فيفري',\n",
              " 'فيم',\n",
              " 'فيما',\n",
              " 'فيه',\n",
              " 'فيها',\n",
              " 'ق',\n",
              " 'قاطبة',\n",
              " 'قاف',\n",
              " 'قام',\n",
              " 'قبل',\n",
              " 'قد',\n",
              " 'قرش',\n",
              " 'قطّ',\n",
              " 'قلما',\n",
              " 'ك',\n",
              " 'كأن',\n",
              " 'كأنما',\n",
              " 'كأنّ',\n",
              " 'كأي',\n",
              " 'كأين',\n",
              " 'كأيّ',\n",
              " 'كأيّن',\n",
              " 'كاد',\n",
              " 'كاف',\n",
              " 'كان',\n",
              " 'كانون',\n",
              " 'كثيرا',\n",
              " 'كذا',\n",
              " 'كذلك',\n",
              " 'كرب',\n",
              " 'كسا',\n",
              " 'كل',\n",
              " 'كلا',\n",
              " 'كلاهما',\n",
              " 'كلتا',\n",
              " 'كلما',\n",
              " 'كليكما',\n",
              " 'كليهما',\n",
              " 'كلَّا',\n",
              " 'كلّما',\n",
              " 'كم',\n",
              " 'كما',\n",
              " 'كن',\n",
              " 'كى',\n",
              " 'كي',\n",
              " 'كيت',\n",
              " 'كيف',\n",
              " 'كيفما',\n",
              " 'كِخ',\n",
              " 'ل',\n",
              " 'لئن',\n",
              " 'لا',\n",
              " 'لا سيما',\n",
              " 'لات',\n",
              " 'لاسيما',\n",
              " 'لام',\n",
              " 'لبيك',\n",
              " 'لدن',\n",
              " 'لدى',\n",
              " 'لست',\n",
              " 'لستم',\n",
              " 'لستما',\n",
              " 'لستن',\n",
              " 'لسن',\n",
              " 'لسنا',\n",
              " 'لعل',\n",
              " 'لعلَّ',\n",
              " 'لعمر',\n",
              " 'لك',\n",
              " 'لكم',\n",
              " 'لكما',\n",
              " 'لكن',\n",
              " 'لكنما',\n",
              " 'لكنَّ',\n",
              " 'لكي',\n",
              " 'لكيلا',\n",
              " 'لم',\n",
              " 'لما',\n",
              " 'لمّا',\n",
              " 'لن',\n",
              " 'لنا',\n",
              " 'له',\n",
              " 'لها',\n",
              " 'لهم',\n",
              " 'لهما',\n",
              " 'لهن',\n",
              " 'لو',\n",
              " 'لولا',\n",
              " 'لوما',\n",
              " 'لي',\n",
              " 'ليت',\n",
              " 'ليرة',\n",
              " 'ليس',\n",
              " 'ليسا',\n",
              " 'ليست',\n",
              " 'ليستا',\n",
              " 'ليسوا',\n",
              " 'م',\n",
              " 'مئة',\n",
              " 'مئتان',\n",
              " 'ما',\n",
              " 'ما أفعله',\n",
              " 'ما انفك',\n",
              " 'ما برح',\n",
              " 'مائة',\n",
              " 'مادام',\n",
              " 'ماذا',\n",
              " 'مارس',\n",
              " 'مازال',\n",
              " 'مافتئ',\n",
              " 'ماي',\n",
              " 'مايو',\n",
              " 'متى',\n",
              " 'مثل',\n",
              " 'مذ',\n",
              " 'مرّة',\n",
              " 'مساء',\n",
              " 'مع',\n",
              " 'معاذ',\n",
              " 'مكانكم',\n",
              " 'مكانكما',\n",
              " 'مكانكنّ',\n",
              " 'مكانَك',\n",
              " 'مليم',\n",
              " 'مما',\n",
              " 'ممن',\n",
              " 'من',\n",
              " 'منذ',\n",
              " 'منه',\n",
              " 'منها',\n",
              " 'مه',\n",
              " 'مهما',\n",
              " 'ميم',\n",
              " 'ن',\n",
              " 'نا',\n",
              " 'نبَّا',\n",
              " 'نحن',\n",
              " 'نحو',\n",
              " 'نعم',\n",
              " 'نفس',\n",
              " 'نوفمبر',\n",
              " 'نون',\n",
              " 'نيسان',\n",
              " 'نيف',\n",
              " 'نَخْ',\n",
              " 'نَّ',\n",
              " 'ه',\n",
              " 'هؤلاء',\n",
              " 'ها',\n",
              " 'هاء',\n",
              " 'هاتان',\n",
              " 'هاته',\n",
              " 'هاتي',\n",
              " 'هاتين',\n",
              " 'هاك',\n",
              " 'هاكَ',\n",
              " 'هاهنا',\n",
              " 'هبّ',\n",
              " 'هذا',\n",
              " 'هذان',\n",
              " 'هذه',\n",
              " 'هذي',\n",
              " 'هذين',\n",
              " 'هكذا',\n",
              " 'هل',\n",
              " 'هلا',\n",
              " 'هللة',\n",
              " 'هلم',\n",
              " 'هلّا',\n",
              " 'هم',\n",
              " 'هما',\n",
              " 'همزة',\n",
              " 'هن',\n",
              " 'هنا',\n",
              " 'هناك',\n",
              " 'هنالك',\n",
              " 'هو',\n",
              " 'هي',\n",
              " 'هيا',\n",
              " 'هيت',\n",
              " 'هيهات',\n",
              " 'هيّا',\n",
              " 'هَؤلاء',\n",
              " 'هَاتانِ',\n",
              " 'هَاتَيْنِ',\n",
              " 'هَاتِه',\n",
              " 'هَاتِي',\n",
              " 'هَجْ',\n",
              " 'هَذا',\n",
              " 'هَذانِ',\n",
              " 'هَذَيْنِ',\n",
              " 'هَذِه',\n",
              " 'هَذِي',\n",
              " 'هَيْهات',\n",
              " 'و',\n",
              " 'وإذ',\n",
              " 'وإذا',\n",
              " 'وإن',\n",
              " 'وا',\n",
              " 'واحد',\n",
              " 'والذي',\n",
              " 'والذين',\n",
              " 'واهاً',\n",
              " 'واو',\n",
              " 'وجد',\n",
              " 'وراءَك',\n",
              " 'ورد',\n",
              " 'ولا',\n",
              " 'ولكن',\n",
              " 'ولو',\n",
              " 'وما',\n",
              " 'ومن',\n",
              " 'وهب',\n",
              " 'وهو',\n",
              " 'وَيْ',\n",
              " 'وُشْكَانَ',\n",
              " 'ى',\n",
              " 'ي',\n",
              " 'يا',\n",
              " 'ياء',\n",
              " 'يفعلان',\n",
              " 'يفعلون',\n",
              " 'يمين',\n",
              " 'ين',\n",
              " 'يناير',\n",
              " 'يوان',\n",
              " 'يورو',\n",
              " 'يوليو',\n",
              " 'يونيو',\n",
              " 'ّأيّان'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the Arabic stop words\n",
        "stop_words = set(stopwords.words('arabic'))\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqxA-_gXmbe9",
        "outputId": "05b558f0-f4c0-4e04-b102-e72c4bbd1188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['قامت', 'الشركة', 'بإصدار', 'بيانات', 'تفصيلية', 'حول', 'أدائها', 'الربع', 'الأول', 'العام', 'الحالي']\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "# Filter out the stop words from the text\n",
        "filtered_words = [word for word in words if not word in stop_words]\n",
        "print(filtered_words)\n",
        "print(len(filtered_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYULVDwEmiTR",
        "outputId": "738434f2-a1a8-453a-d03d-dd0b52df134c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "قامت الشركة بإصدار بيانات تفصيلية حول أدائها الربع الأول العام الحالي\n"
          ]
        }
      ],
      "source": [
        "# Join the filtered words into a string\n",
        "filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "# Print the filtered text\n",
        "print(filtered_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2G9S9SamtU5"
      },
      "source": [
        "##stop words in english"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56bz9SkEnETG",
        "outputId": "cad62d6e-3264-4324-fbad-a68fb6f0fceb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGvHDE2nmonB",
        "outputId": "4fea65a5-68af-4ef7-8ec6-875a27c5a6fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'how', 'are', 'you', 'doing', '?', 'i', 'hope', 'you', 'are', 'doing', 'well', '.']\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "# Define a sample English text\n",
        "english_text = \"Hello, how are you doing? i hope you are doing well.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = word_tokenize(english_text)\n",
        "print(words)\n",
        "print(len(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6iggpEInLJQ",
        "outputId": "923de9ff-255f-4511-bc40-539e52d2d65e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', '?', 'hope', 'well', '.']\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "# Load the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out the stop words from the text\n",
        "filtered_words = [word for word in words if not word in stop_words]\n",
        "\n",
        "print(filtered_words)\n",
        "print(len(filtered_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVyDVaoknRjB",
        "outputId": "5b2eca0c-8f4e-4bc4-a18d-fb67aa5cba91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello , ? hope well .\n"
          ]
        }
      ],
      "source": [
        "# Join the filtered words into a string\n",
        "filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "# Print the filtered text\n",
        "print(filtered_text)\n",
        "\n",
        "#you can exlude some words in stop of words if you see the result nit good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTDrLBHjpn8s"
      },
      "source": [
        "#Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YisCQ6DLprs8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHqqt78BptvB",
        "outputId": "97fd0af3-94e9-4526-fcd4-8eec9e8af3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "walking -> walk\n",
            "jumps -> jump\n",
            "jumped -> jump\n",
            "jumping -> jump\n"
          ]
        }
      ],
      "source": [
        "# Create a stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem some example words\n",
        "words = [\"walking\", \"jumps\", \"jumped\", \"jumping\"]\n",
        "for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    print(f\"{word} -> {stemmed_word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbMYT37ht1d7"
      },
      "source": [
        "#Lemitaziation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et-zZwrYu3CQ",
        "outputId": "7163f095-6038-4cf0-8cd9-ce79765fc690"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2IVzO5Ct3pL",
        "outputId": "51965c94-81eb-4e4c-ea0b-9cafa6dd42c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog']\n",
            "['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The quick brown foxes jumped over the lazy dogs\"\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "print(stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
        "print(lemmatized_words)\n",
        "\n",
        "#look for the lazy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wbxKyNg4StP"
      },
      "source": [
        "#regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdf50q1o57q4"
      },
      "source": [
        "##extract a digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qk4roqao4X_-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text='''\n",
        "Elon musk's phone number is 9991116666, call him if you have any questions on dodgecoin.\n",
        "Tesla's revenue is 40 billion\n",
        "Tesla's CFO number (999)-333-7777\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktCbC-P04Yhn",
        "outputId": "9d96ce58-e732-4ca1-cd02-f5efb9845fba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['999', '111', '666', '999', '333', '777']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = '\\d{3}'\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVkPqi1T4mf0",
        "outputId": "df8530bd-3c0b-403d-b8b1-85aae4c51008"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(999)-333-7777']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = '\\(\\d{3}\\)-\\d{3}-\\d{4}'\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MuFjWG9njwe",
        "outputId": "b68cc530-a8cb-4795-8d85-57b30560ae78"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['9991116666', '(999)-333-7777']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "pattern = '\\(\\d{3}\\)-\\d{3}-\\d{4}|\\d{10}'\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdZqpM3r5_rY"
      },
      "source": [
        "##extract a head line\n",
        "\n",
        "1- Note 1 - Overview\n",
        "\n",
        "2- Note 2 - Summary of Significant Accounting Policies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KhYdaw06B36"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "Note 1 - Overview\n",
        "Tesla, Inc. (“Tesla”, the “Company”, “we”, “us” or “our”) was incorporated in the State of Delaware on July 1, 2003. We design, develop, manufacture and sell high-performance fully electric vehicles and design, manufacture, install and sell solar energy generation and energy storage\n",
        "products. Our Chief Executive Officer, as the chief operating decision maker (“CODM”), organizes our company, manages resource allocations and measures performance among two operating and reportable segments: (i) automotive and (ii) energy generation and storage.\n",
        "Beginning in the first quarter of 2021, there has been a trend in many parts of the world of increasing availability and administration of vaccines\n",
        "against COVID-19, as well as an easing of restrictions on social, business, travel and government activities and functions. On the other hand, infection\n",
        "rates and regulations continue to fluctuate in various regions and there are ongoing global impacts resulting from the pandemic, including challenges\n",
        "and increases in costs for logistics and supply chains, such as increased port congestion, intermittent supplier delays and a shortfall of semiconductor\n",
        "supply. We have also previously been affected by temporary manufacturing closures, employment and compensation adjustments and impediments to\n",
        "administrative activities supporting our product deliveries and deployments.\n",
        "Note 2 - Summary of Significant Accounting Policies\n",
        "Unaudited Interim Financial Statements\n",
        "The consolidated balance sheet as of September 30, 2021, the consolidated statements of operations, the consolidated statements of\n",
        "comprehensive income, the consolidated statements of redeemable noncontrolling interests and equity for the three and nine months ended September\n",
        "30, 2021 and 2020 and the consolidated statements of cash flows for the nine months ended September 30, 2021 and 2020, as well as other information\n",
        "disclosed in the accompanying notes, are unaudited. The consolidated balance sheet as of December 31, 2020 was derived from the audited\n",
        "consolidated financial statements as of that date. The interim consolidated financial statements and the accompanying notes should be read in\n",
        "conjunction with the annual consolidated financial statements and the accompanying notes contained in our Annual Report on Form 10-K for the year\n",
        "ended December 31, 2020.\n",
        "'''\n",
        "\n",
        "\n",
        "#.>>>> distance\n",
        "#[^a]>>>exclude the a character\n",
        "#*>>>>>> the rest of characters\n",
        "#\\n >>>> the new line\n",
        "#note(x)>>> the result is only x without note"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8JCa2lS77kh",
        "outputId": "24c13e95-0b00-4410-dd68-f878c81b1ba5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Note 1 - Overview', 'Note 2 - Summary of Significant Accounting Policies']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = 'Note.\\d.-.[^\\n]*'\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpyoKKOe6C37",
        "outputId": "2f4d4b35-808c-4b00-c5d2-a4038c9722fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Overview', 'Summary of Significant Accounting Policies']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = 'Note.\\d.-.([^\\n]*)'\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5RAWJj_9wQh"
      },
      "source": [
        "##Extract that \"['FY2021 Q1', 'fy2020 Q4']\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs8S1xgC5K09"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "The gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "In previous quarter i.e. fy2020 Q4 it was $3 billion fy2020 Q5.\n",
        "'''\n",
        "#['FY2021 Q1', 'fy2020 Q4']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvCtD8FH81QW",
        "outputId": "3df851b3-a7e7-4d1a-a7aa-ab36622f2fa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['FY2021 Q1', 'fy2020 Q4']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pattern = 'FY\\d{4} Q[1-4]'\n",
        "\n",
        "matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx9X26LW-yuy"
      },
      "source": [
        "##Extract only financial numbers\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOhbrzac9DAl"
      },
      "outputs": [],
      "source": [
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV2vp0xx-2tG",
        "outputId": "0e8cd09e-5ff2-45e7-da18-3b0004ec6690"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['$4.85', '$3']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ( $4.85 , $3)\n",
        "\n",
        "pattern = '\\$[0-9\\.]+'\n",
        "\n",
        "matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "matches\n",
        "\n",
        "\n",
        "# $ is special character, if you want $ as it write that \\$\n",
        "# . referes to any thing, if you want . as it write that \\."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLGMziYHu5z_",
        "outputId": "cb19bfbf-781b-4994-d51e-b606f9dbc024"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['4.85', '3']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ( 4.85 , 3)  >>> use () to exclude the $\n",
        "\n",
        "pattern = '\\$([0-9\\.]+)'\n",
        "\n",
        "matches = re.findall(pattern, text, flags=re.IGNORECASE)\n",
        "matches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT478mmsw6nI"
      },
      "source": [
        "##Extract periods and financial numbers both\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPHuZ7Slwpjv",
        "outputId": "219ef10e-c2a0-4a33-db3e-9b492e4010a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['FY2021 Q1 was $4.85', 'FY2020 Q4 it was $3']"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ['FY2021 Q1 was $4.85', 'FY2020 Q4 it was $3']\n",
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
        "'''\n",
        "pattern ='(FY\\d{4}.Q[1-4][^\\$]+\\$[0-9\\.]+)'\n",
        "# pattern ='FY(\\d{4} Q[1-4])[^\\$]+\\$([0-9\\.]+)'\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "matches\n",
        "\n",
        "# ( ) seperate the value which in () about other\n",
        "# [^a] exclude a character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QhuktdSw8o8",
        "outputId": "133dd88e-7513-4dbf-bbbe-56a54e997e71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('2021 Q1', '4.85'), ('2020 Q4', '3')]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [('2021 Q1', '4.85'), ('2020 Q4', '3')]\n",
        "\n",
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
        "'''\n",
        "pattern ='FY(\\d{4}.Q[1-4])[^\\$]+\\$([0-9\\.]+)'\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUrhx_p53hrK"
      },
      "source": [
        "##search pattern\n",
        "\n",
        "1- search  >>>>>return the first pattern achieve the condition.\n",
        "\n",
        "2- findall  >>>>>return all the patterns achieve the condition\n",
        "\n",
        "3- use groups() to print it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-61hs153kgk",
        "outputId": "764f9145-ecd3-43ef-9db7-163a02a4cbeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('2021 Q1', '4.85')"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [('2021 Q1', '4.85'), ('2020 Q4', '3')]\n",
        "\n",
        "text = '''\n",
        "Tesla's gross cost of operating lease vehicles in FY2021 Q1 was $4.85 billion.\n",
        "In previous quarter i.e. FY2020 Q4 it was $3 billion.\n",
        "'''\n",
        "pattern ='FY(\\d{4}.Q[1-4])[^\\$]+\\$([0-9\\.]+)'\n",
        "\n",
        "matches = re.search(pattern, text)\n",
        "matches.groups()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnzqQ88W4SI_"
      },
      "source": [
        "##Task1\n",
        "\n",
        "Extract all twitter handles from following text. Twitter handle is the text that appears after https://twitter.com/ and is a single word. Also it contains only alpha numeric characters i.e. A-Z a-z , o to 9 and underscore _"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgnV11ms4UAK",
        "outputId": "85b5f9fe-9b1a-4a6a-f8d5-5d2e07a250cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['elonmusk', 'teslarati', 'dummy_tesla', 'dummy_2_tesla']"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = '''\n",
        "Follow our leader Elon musk on twitter here: https://twitter.com/elonmusk, more information\n",
        "on Tesla's products can be found at https://www.tesla.com/. Also here are leading influencers\n",
        "for tesla related news,\n",
        "https://twitter.com/teslarati\n",
        "https://twitter.com/dummy_tesla\n",
        "https://twitter.com/dummy_2_tesla\n",
        "'''\n",
        "pattern = 'https:\\/\\/twitter\\.com\\/([A-Za-z0-9_]+)'\n",
        "\n",
        "re.findall(pattern, text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKnbCQnY7pDX"
      },
      "source": [
        "##Task2\n",
        "Extract Concentration Risk Types. It will be a text that appears after \"Concentration Risk:\", In below example, your regex should extract these two strings\n",
        "\n",
        "(1) Credit Risk\n",
        "\n",
        "(2) Supply Rish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXiyYuyH2A72",
        "outputId": "1e23822d-1a7e-46a2-d720-242c5d890e4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Credit Risk', 'Supply Risk']"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = '''\n",
        "Concentration of Risk: Credit Risk\n",
        "Financial instruments that potentially subject us to a concentration of credit risk consist of cash, cash equivalents, marketable securities,\n",
        "restricted cash, accounts receivable, convertible note hedges, and interest rate swaps. Our cash balances are primarily invested in money market funds\n",
        "or on deposit at high credit quality financial institutions in the U.S. These deposits are typically in excess of insured limits. As of September 30, 2021\n",
        "and December 31, 2020, no entity represented 10% or more of our total accounts receivable balance. The risk of concentration for our convertible note\n",
        "hedges and interest rate swaps is mitigated by transacting with several highly-rated multinational banks.\n",
        "Concentration of Risk: Supply Risk\n",
        "We are dependent on our suppliers, including single source suppliers, and the inability of these suppliers to deliver necessary components of our\n",
        "products in a timely manner at prices, quality levels and volumes acceptable to us, or our inability to efficiently manage these components from these\n",
        "suppliers, could have a material adverse effect on our business, prospects, financial condition and operating results.\n",
        "'''\n",
        "# pattern = 'Concentration of Risk: ([A-Za-z]+.[[A-Za-z]+)'\n",
        "pattern ='Concentration of Risk: (\\w+\\s*\\w*)'\n",
        "\n",
        "re.findall(pattern, text)\n",
        "\n",
        "# \\w small   >>> equal to [a-zA-Z0-9_]\n",
        "# \\W capital >>> the inverse of \\w small\n",
        "# +          >>> one or more\n",
        "# \\s small   >>> any whitespace character\n",
        "# \\S capital >>> not whitespace character\n",
        "# + one or more\n",
        "# * zero or more"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task3"
      ],
      "metadata": {
        "id": "6rHliKLV2mco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text='my name is ali and my salary : 10000'\n",
        "pattern ='salary.:.([0-9]+)'\n",
        "print(re.findall(pattern, text))\n",
        "print(re.sub(pattern,'20000',text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiUgpz232ouf",
        "outputId": "c80519a8-558e-489d-9797-9136d38caf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10000']\n",
            "my name is ali and my 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xKEtJzaN3RQ"
      },
      "source": [
        "#POS(Part of speech)\n",
        "\n",
        "1- token.pos_ >>>>>>>represent pos(verp, noun,....)\n",
        "\n",
        "2- spacy.explain(token.pos_)   >>>>>akes the part-of-speech tag as an argument and returns a brief explanation of what the pos represents.\n",
        "\n",
        "3- token.tag_  >>>> tell me the tense(الزمن  بتاعى )\n",
        "\n",
        "4- spacy.explain(token.tag_ ) >>>> returns a brief explanation of what the tag represents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftq4Ywg5N6bO"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# en_core_web_sm  >>>>>> pre-trained model is designed for processing English text for tasks such as\n",
        "# part-of-speech tagging, named entity recognition, and dependency parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxs8IJSUOp52",
        "outputId": "bfa4fe41-d102-41b6-fb57-2abfd4302822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elon || PROPN || proper noun\n",
            "flew || VERB || verb\n",
            "to || ADP || adposition\n",
            "mars || NOUN || noun\n",
            "yesterday || NOUN || noun\n",
            ". || PUNCT || punctuation\n",
            "He || PRON || pronoun\n",
            "carried || VERB || verb\n",
            "biryani || ADJ || adjective\n",
            "masala || NOUN || noun\n",
            "with || ADP || adposition\n",
            "him || PRON || pronoun\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token,\"||\", token.pos_,\"||\", spacy.explain(token.pos_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2yrHETrOyKs",
        "outputId": "cb104c61-5343-4272-c202-40c8b94a6566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wow  |  INTJ  |  interjection  |  UH  |  interjection\n",
            "!  |  PUNCT  |  punctuation  |  .  |  punctuation mark, sentence closer\n",
            "Dr.  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "Strange  |  PROPN  |  proper noun  |  NNP  |  noun, proper singular\n",
            "made  |  VERB  |  verb  |  VBD  |  verb, past tense\n",
            "265  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "million  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "$  |  NUM  |  numeral  |  CD  |  cardinal number\n",
            "on  |  ADP  |  adposition  |  IN  |  conjunction, subordinating or preposition\n",
            "the  |  DET  |  determiner  |  DT  |  determiner\n",
            "very  |  ADV  |  adverb  |  RB  |  adverb\n",
            "first  |  ADJ  |  adjective  |  JJ  |  adjective (English), other noun-modifier (Chinese)\n",
            "day  |  NOUN  |  noun  |  NN  |  noun, singular or mass\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token,\" | \", token.pos_, \" | \", spacy.explain(token.pos_), \" | \", token.tag_, \" | \", spacy.explain(token.tag_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBgOLzIrSH5O"
      },
      "source": [
        "##using pos as a filter for Removing all SPACE, PUNCT and X token from text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebyI3fhrQP7r"
      },
      "outputs": [],
      "source": [
        "earnings_text=\"\"\"Microsoft Corp. today announced the following results for the quarter ended December 31, 2021, as compared to the corresponding period of last fiscal year:\n",
        "\n",
        "·         Revenue was $51.7 billion and increased 20%\n",
        "·         Operating income was $22.2 billion and increased 24%\n",
        "·         Net income was $18.8 billion and increased 21%\n",
        "·         Diluted earnings per share was $2.48 and increased 22%\n",
        "“Digital technology is the most malleable resource at the world’s disposal to overcome constraints and reimagine everyday work and life,” said Satya Nadella, chairman and chief executive officer of Microsoft. “As tech as a percentage of global GDP continues to increase, we are innovating and investing across diverse and growing markets, with a common underlying technology stack and an operating model that reinforces a common strategy, culture, and sense of purpose.”\n",
        "“Solid commercial execution, represented by strong bookings growth driven by long-term Azure commitments, increased Microsoft Cloud revenue to $22.1 billion, up 32% year over year” said Amy Hood, executive vice president and chief financial officer of Microsoft.\"\"\"\n",
        "\n",
        "doc = nlp(earnings_text)\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ not in [\"SPACE\", \"PUNCT\", \"X\"]:\n",
        "        filtered_tokens.append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXXpgsmASpIo",
        "outputId": "cf77a474-ccbf-4bf7-9d64-a621b1c1994a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Microsoft,\n",
              " Corp.,\n",
              " today,\n",
              " announced,\n",
              " the,\n",
              " following,\n",
              " results,\n",
              " for,\n",
              " the,\n",
              " quarter]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_tokens[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMCQUGPRSsZG"
      },
      "source": [
        "##get the count of each pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUTm6JaKSvJH",
        "outputId": "afc23b42-a497-4504-fb74-2776138525e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{96: 13,\n",
              " 92: 46,\n",
              " 100: 24,\n",
              " 90: 9,\n",
              " 85: 16,\n",
              " 93: 16,\n",
              " 97: 27,\n",
              " 98: 1,\n",
              " 84: 20,\n",
              " 103: 10,\n",
              " 87: 6,\n",
              " 99: 5,\n",
              " 89: 12,\n",
              " 86: 3,\n",
              " 94: 3,\n",
              " 95: 2}"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count = doc.count_by(spacy.attrs.POS)\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4acloghmSpuB",
        "outputId": "136d2774-1059-4c15-d74f-ddcb8c535e0a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'PROPN'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc.vocab[96].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A36JhehOS2-U",
        "outputId": "fc8f1c6e-61e3-4d4a-cb7c-4c8e78fe94ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([(96, 13), (92, 46), (100, 24), (90, 9), (85, 16), (93, 16), (97, 27), (98, 1), (84, 20), (103, 10), (87, 6), (99, 5), (89, 12), (86, 3), (94, 3), (95, 2)])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQD8e5Y9S4Zs",
        "outputId": "f6e36cf5-c0c5-4fa1-e920-09b888bfae1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROPN | 13\n",
            "NOUN | 46\n",
            "VERB | 24\n",
            "DET | 9\n",
            "ADP | 16\n",
            "NUM | 16\n",
            "PUNCT | 27\n",
            "SCONJ | 1\n",
            "ADJ | 20\n",
            "SPACE | 10\n",
            "AUX | 6\n",
            "SYM | 5\n",
            "CCONJ | 12\n",
            "ADV | 3\n",
            "PART | 3\n",
            "PRON | 2\n"
          ]
        }
      ],
      "source": [
        "for k,v in count.items():\n",
        "    print(doc.vocab[k].text, \"|\",v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu6v1JbMS_12"
      },
      "source": [
        "##apply pos using ntlk not spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFka3tgxTC0_",
        "outputId": "16decc66-f7f8-4691-fe99-f18308c70581"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The DT\n",
            "quick JJ\n",
            "brown NN\n",
            "fox NN\n",
            "jumps VBZ\n",
            "over IN\n",
            "the DT\n",
            "lazy JJ\n",
            "dog NN\n",
            ". .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging on the tokenized words\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Print the POS tags for each word\n",
        "for word, tag in pos_tags:\n",
        "    print(word, tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjGt0FRfi8w1"
      },
      "source": [
        "#Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzcGmYKzjCCv"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# en_core_web_sm >>> small model\n",
        "# en_core_web_md >>> meduim model\n",
        "# en_core_web_lg >>> large model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC9E-hcyS9Gl",
        "outputId": "4c55b583-dd90-4705-ec2a-2ecf02df23ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nlp.pipe_names\n",
        "#the tasks which it will do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwhelwYFjQLf",
        "outputId": "6fb8e5e0-31ba-4ad7-a2e7-de9962ef3d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
            "$45 billion  |  MONEY  |  Monetary values, including unit\n"
          ]
        }
      ],
      "source": [
        "doc1 = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "qyPF7DfqkaMe",
        "outputId": "5e4f0041-4b83-4721-d215-d343040cbbac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    Tesla Inc\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\\n</mark>\\n is going to acquire twitter for \\n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\\n    $45 billion\\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\\n</mark>\\n</div>'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc1, style=\"ent\")\n",
        "\n",
        "# https://onecompiler.com/html/3zec3j8sc  >>> to show the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-VjgjNAkanw",
        "outputId": "9153a19e-8fca-4f01-91da-5065a6b86f68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['CARDINAL',\n",
              " 'DATE',\n",
              " 'EVENT',\n",
              " 'FAC',\n",
              " 'GPE',\n",
              " 'LANGUAGE',\n",
              " 'LAW',\n",
              " 'LOC',\n",
              " 'MONEY',\n",
              " 'NORP',\n",
              " 'ORDINAL',\n",
              " 'ORG',\n",
              " 'PERCENT',\n",
              " 'PERSON',\n",
              " 'PRODUCT',\n",
              " 'QUANTITY',\n",
              " 'TIME',\n",
              " 'WORK_OF_ART']"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##List down all the entities\n",
        "nlp.pipe_labels['ner']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgPKd8VxmWDR",
        "outputId": "f8e59f7e-b401-4021-e6fc-a6cd1b86d73a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla Inc  |  ORG  |  0 | 9\n",
            "Twitter Inc  |  ORG  |  30 | 41\n",
            "$45 billion  |  MONEY  |  46 | 57\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire Twitter Inc for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_, \" | \", ent.start_char, \"|\", ent.end_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA4azKAKjtjO",
        "outputId": "059867e5-667c-4ddc-c3f5-b2791ef41ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla  |  ORG\n",
            "Twitter  |  PRODUCT\n",
            "$45 billion  |  MONEY\n"
          ]
        }
      ],
      "source": [
        "#Setting custom entities\n",
        "doc = nlp(\"Tesla is going to acquire Twitter for $45 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_)\n",
        "\n",
        "\n",
        "#here, when we deleted inc after twitter, the result was twiiter is product and that uncorrect so i need to handle in manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQg974q3nO28",
        "outputId": "25aab989-f575-4d87-fc9c-11aa12f1d9e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "going to acquire"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s = doc[2:5]\n",
        "s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrO4WbQYnK-c",
        "outputId": "2da862cd-c5e0-427d-b743-da7528456cef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CucGiAJnEkt"
      },
      "outputs": [],
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "s1 = Span(doc, 0, 1, label=\"ORG\")\n",
        "s2 = Span(doc, 5, 6, label=\"ORG\")\n",
        "\n",
        "doc.set_ents([s1, s2], default=\"unmodified\")\n",
        "# save the changes in s1,s2 without modification in others indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJoMW6QEm2wY",
        "outputId": "c408c420-2bec-487e-8c89-c7d33b67b12d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesla  |  ORG\n",
            "Twitter  |  ORG\n",
            "$45 billion  |  MONEY\n"
          ]
        }
      ],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, \" | \", ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFec2wMNoy16"
      },
      "source": [
        "##using NLTK\n",
        "\n",
        "\n",
        "1- ne_chunk >>> responsible for NER IN NLTK which take tags as an input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3PnFzxJniPa",
        "outputId": "67a9688a-dec1-4cf2-843f-c399f93f9662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "pos_tags [('Steve', 'NNP'), ('Jobs', 'NNP'), ('was', 'VBD'), ('the', 'DT'), ('CEO', 'NNP'), ('of', 'IN'), ('Apple', 'NNP'), ('Corp.', 'NNP'), ('in', 'IN'), ('California', 'NNP'), ('.', '.')]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ne_tree (S\n",
            "  (PERSON Steve/NNP)\n",
            "  (PERSON Jobs/NNP)\n",
            "  was/VBD\n",
            "  the/DT\n",
            "  (ORGANIZATION CEO/NNP)\n",
            "  of/IN\n",
            "  (ORGANIZATION Apple/NNP Corp./NNP)\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Define input text\n",
        "input_text = \"Steve Jobs was the CEO of Apple Corp. in California.\"\n",
        "\n",
        "# Tokenize input text\n",
        "tokens = word_tokenize(input_text)\n",
        "\n",
        "# Perform Part-of-Speech (POS) tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print('-'*100)\n",
        "print('pos_tags',pos_tags)\n",
        "print('-'*100)\n",
        "\n",
        "# Perform Named Entity Recognition (NER)\n",
        "ne_tree = ne_chunk(pos_tags)\n",
        "print('ne_tree',ne_tree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhZhMljyo58_",
        "outputId": "1b954449-ffad-4e6c-8e1e-7a998b5f3e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Steve\n",
            "Jobs\n",
            "CEO\n",
            "Apple Corp.\n",
            "[('Steve', 'PERSON'), ('Jobs', 'PERSON'), ('CEO', 'ORGANIZATION'), ('Apple Corp.', 'ORGANIZATION')]\n"
          ]
        }
      ],
      "source": [
        "# Extract named entities and their labels\n",
        "named_entities = []\n",
        "for subtree in ne_tree.subtrees():\n",
        "    if subtree.label() in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
        "        named_entity = ' '.join(word for word, tag in subtree.leaves())\n",
        "        print(named_entity)\n",
        "        named_entities.append((named_entity, subtree.label()))\n",
        "\n",
        "# Print named entities and their labels\n",
        "print(named_entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sh9mucGwpd22",
        "outputId": "7563faa7-7f8c-44e6-bd1b-2fe15be69596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "California\n"
          ]
        }
      ],
      "source": [
        "for word, tag in subtree.leaves():\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXZ3x4YB1mTP"
      },
      "source": [
        "#spell correction\n",
        "\n",
        "1- using spellchecker()\n",
        "\n",
        "2- using textblob()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VGhm30D1o3m",
        "outputId": "6a9a2ce4-90fb-47cc-d0ca-bff34ed6535f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I have a good spelling!\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Create a TextBlob object with the text to correct\n",
        "text = \"I havv a goood speling!\"\n",
        "\n",
        "# Create a new TextBlob object with corrected spelling\n",
        "corrected_text = TextBlob(text).correct()\n",
        "\n",
        "# Print the corrected text\n",
        "print(corrected_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdGK-oL2bS5B"
      },
      "source": [
        "#BOW(Bag of words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q68JmMh1bWqm"
      },
      "outputs": [],
      "source": [
        "paragraph =  \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVMFl6ebjbZ",
        "outputId": "43165c11-8f1c-4faf-fe4f-d9fe7c11be5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWA7o_b1DYXg",
        "outputId": "40d34b78-33a3-4132-e28a-7bf417322709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['three vision india',\n",
              " 'year history people world come invaded u captured land conquered mind',\n",
              " 'alexander onwards greek turk mogul portuguese british french dutch came looted u took',\n",
              " 'yet done nation',\n",
              " 'conquered anyone',\n",
              " 'grabbed land culture history tried enforce way life',\n",
              " '',\n",
              " 'respect freedom others first vision freedom',\n",
              " 'believe india got first vision started war independence',\n",
              " 'freedom must protect nurture build',\n",
              " 'free one respect u',\n",
              " 'second vision india development',\n",
              " 'fifty year developing nation',\n",
              " 'time see developed nation',\n",
              " 'among top nation world term gdp',\n",
              " 'percent growth rate area',\n",
              " 'poverty level falling',\n",
              " 'achievement globally recognised today',\n",
              " 'yet lack self confidence see developed nation self reliant self assured',\n",
              " 'incorrect',\n",
              " 'third vision',\n",
              " 'india must stand world',\n",
              " 'believe unless india stand world one respect u',\n",
              " 'strength respect strength',\n",
              " 'must strong military power also economic power',\n",
              " 'must go hand hand',\n",
              " 'good fortune worked three great mind',\n",
              " 'dr vikram sarabhai dept',\n",
              " 'space professor satish dhawan succeeded dr brahm prakash father nuclear material',\n",
              " 'lucky worked three closely consider great opportunity life',\n",
              " 'see four milestone career']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cleaning the texts\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemitizer=WordNetLemmatizer()\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph) #split the paragraph to individual sentences based on spaces.\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for i in range(len(sentences)): ##loop for each sentences individually\n",
        "\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #replace any thing othwerwise characters with space\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    #review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))] #bad result\n",
        "    review = [lemitizer.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "print(len(corpus))\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCbCtVJQfcDu",
        "outputId": "359c900e-4b74-4833-dfb3-3a59dbe183d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<31x114 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 155 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Creating the Bag of Words model\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv = CountVectorizer()\n",
        "# x = cv.fit_transform(corpus)\n",
        "# x\n",
        "\n",
        "# #but x is still in sparse matrix shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQTpcEwGHZi4",
        "outputId": "90636abd-776a-4376-a1c4-112016c8f42a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(31, 114)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 0],\n",
              "       [0, 1, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "x = cv.fit_transform(corpus).toarray()\n",
        "print(x.shape)\n",
        "x\n",
        "#so we have 114 unique words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA0knvJadaIx"
      },
      "source": [
        "#N-GRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxU5AJljkEis"
      },
      "source": [
        "##UNi-GRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9490468a",
        "outputId": "24e4c36f-7101-493f-cd34-ccbb9696290e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "v = CountVectorizer()\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_\n",
        "\n",
        "# give each word individually and it's index as a column\n",
        "# by default(one gram)\n",
        "# using fit function not fit_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a2697fc",
        "outputId": "c2e00e31-0120-418c-86e9-0b2ab84d1c10",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thor': 5, 'hathodawala': 1, 'is': 2, 'looking': 4, 'for': 0, 'job': 3}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = CountVectorizer(ngram_range=(1,1))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fja1Hz8xkLCf"
      },
      "source": [
        "##BI-GRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "132a19d8",
        "outputId": "0e85a3c0-4921-4558-c283-68a1799d54f1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thor': 9,\n",
              " 'hathodawala': 2,\n",
              " 'is': 4,\n",
              " 'looking': 7,\n",
              " 'for': 0,\n",
              " 'job': 6,\n",
              " 'thor hathodawala': 10,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 5,\n",
              " 'looking for': 8,\n",
              " 'for job': 1}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = CountVectorizer(ngram_range=(1,2))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3b3d05f",
        "outputId": "51f92740-a5bb-461c-e88e-7703edf6001e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thor': 12,\n",
              " 'hathodawala': 2,\n",
              " 'is': 5,\n",
              " 'looking': 9,\n",
              " 'for': 0,\n",
              " 'job': 8,\n",
              " 'thor hathodawala': 13,\n",
              " 'hathodawala is': 3,\n",
              " 'is looking': 6,\n",
              " 'looking for': 10,\n",
              " 'for job': 1,\n",
              " 'thor hathodawala is': 14,\n",
              " 'hathodawala is looking': 4,\n",
              " 'is looking for': 7,\n",
              " 'looking for job': 11}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = CountVectorizer(ngram_range=(1,3))\n",
        "v.fit([\"Thor Hathodawala is looking for a job\"])\n",
        "v.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT_sLD4lkvTY"
      },
      "source": [
        "##Task :\n",
        "\n",
        "1- remove stop words\n",
        "\n",
        "2- lemmatize the text\n",
        "\n",
        "3- apply n-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ca4ead"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Thor ate pizza\",\n",
        "    \"Loki is tall\",\n",
        "    \"Loki is eating pizza\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8af5e103"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# load english language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess(text):\n",
        "    # remove stop words and lemmatize the text\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = []\n",
        "    for token in doc:\n",
        "        if token.is_stop or token.is_punct:\n",
        "            continue\n",
        "        filtered_tokens.append(token.lemma_)\n",
        "\n",
        "    return \" \".join(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "99c61291",
        "outputId": "06802b7e-8f3b-4350-ecaa-a26076b6e25b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'thor eat pizza'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess(\"Thor ate pizza\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a4b3eb32",
        "outputId": "039e25ea-075c-4e1a-983e-202156585e5a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Loki eat pizza'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocess(\"Loki is eating pizza\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b315340d",
        "outputId": "1da2dbee-e124-4e3f-c78e-9ca84450b6cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['thor eat pizza', 'Loki tall', 'Loki eat pizza']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_processed = [\n",
        "    preprocess(text) for text in corpus\n",
        "]\n",
        "corpus_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cfed6e3",
        "outputId": "f6deeaec-93e0-4e2f-b6d7-8be9cfe69ee8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'thor': 7,\n",
              " 'eat': 0,\n",
              " 'pizza': 5,\n",
              " 'thor eat': 8,\n",
              " 'eat pizza': 1,\n",
              " 'loki': 2,\n",
              " 'tall': 6,\n",
              " 'loki tall': 4,\n",
              " 'loki eat': 3}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v = CountVectorizer(ngram_range=(1,2))\n",
        "v.fit(corpus_processed)\n",
        "v.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be1e8368",
        "outputId": "e9334d30-8547-4140-be36-d709cfb12205"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 0, 1, 1]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fit >>>> take all the documents.\n",
        "# transform  >>> take sentence that you want to encode it.\n",
        "v.transform([\"Thor eat pizza\"]).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5a9ac22",
        "outputId": "80d9c4ac-3134-42a5-af0a-2e6acbc46d14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 0, 0, 0]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "v.transform([\"Hulk eat pizza\"]).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbmAW3Mo2LDh"
      },
      "source": [
        "#TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDoJsrAi2bVv"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK55auFW2cdG"
      },
      "outputs": [],
      "source": [
        "# Example documents\n",
        "docs = [\n",
        "  \"The quick brown fox jumps over the lazy dog.\",\n",
        "  \"A stitch in time saves nine.\",\n",
        "  \"The early bird catches the worm.\",\n",
        "  \"Actions speak louder than words.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2JWdu6V2OaI",
        "outputId": "a8a6ceeb-7c1f-4dc5-adbb-4c19d33135e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.32467583 0.         0.32467583 0.\n",
            "  0.32467583 0.         0.32467583 0.32467583 0.         0.\n",
            "  0.32467583 0.32467583 0.         0.         0.         0.\n",
            "  0.5119563  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.4472136  0.         0.         0.         0.4472136\n",
            "  0.         0.         0.4472136  0.         0.4472136  0.\n",
            "  0.         0.4472136  0.         0.        ]\n",
            " [0.         0.39264414 0.         0.39264414 0.         0.39264414\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.6191303  0.         0.         0.39264414]\n",
            " [0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.4472136  0.\n",
            "  0.         0.         0.         0.4472136  0.         0.4472136\n",
            "  0.         0.         0.4472136  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Create a TF-IDF vectorizer instance\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the documents\n",
        "tfidf.fit(docs)\n",
        "\n",
        "# Transform the documents into a TF-IDF matrix\n",
        "tfidf_matrix = tfidf.transform(docs)\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# YOU CAN MAKE IT IN ONE LINE using fit_transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atTQhcad5ltL"
      },
      "source": [
        "When sklearn.__version__ <= 0.24.x use following method\n",
        "\n",
        "get_feature_names()\n",
        "\n",
        "\n",
        "When sklearn.__version__ >= 1.0.x use following method\n",
        "\n",
        "get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9x3Xhgx4p4V",
        "outputId": "bec93819-f7e9-4985-fa85-7344155c4285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    actions      bird     brown   catches       dog     early       fox  \\\n",
            "0  0.000000  0.000000  0.324676  0.000000  0.324676  0.000000  0.324676   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.000000  0.392644  0.000000  0.392644  0.000000  0.392644  0.000000   \n",
            "3  0.447214  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "         in     jumps      lazy  ...      over     quick     saves     speak  \\\n",
            "0  0.000000  0.324676  0.324676  ...  0.324676  0.324676  0.000000  0.000000   \n",
            "1  0.447214  0.000000  0.000000  ...  0.000000  0.000000  0.447214  0.000000   \n",
            "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.447214   \n",
            "\n",
            "     stitch      than       the      time     words      worm  \n",
            "0  0.000000  0.000000  0.511956  0.000000  0.000000  0.000000  \n",
            "1  0.447214  0.000000  0.000000  0.447214  0.000000  0.000000  \n",
            "2  0.000000  0.000000  0.619130  0.000000  0.000000  0.392644  \n",
            "3  0.000000  0.447214  0.000000  0.000000  0.447214  0.000000  \n",
            "\n",
            "[4 rows x 22 columns]\n"
          ]
        }
      ],
      "source": [
        "# Convert the matrix to a pandas dataframe\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
        "\n",
        "# Display the dataframe\n",
        "print(df_tfidf)\n",
        "\n",
        "\n",
        "# will make every unique word as a feature, so because i have 22 columns.\n",
        "# noticed, the most frequency word is the least value of TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## the numbers of uniques words(min_df)\n",
        "\n",
        "1- min_df>>>>>>> will control the numbers of words which will be appeared based on numbers of frequency"
      ],
      "metadata": {
        "id": "ojS4YjMvFJqJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS9UECmk42NK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca41ee10-955e-41ca-c841-a57e16aafa9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown' 'cat' 'dog' 'fox' 'lazy' 'quick' 'the']\n",
            "[[0.4804584  0.         0.         0.63174505 0.         0.4804584\n",
            "  0.37311881]\n",
            " [0.         0.         0.65249088 0.         0.65249088 0.\n",
            "  0.38537163]\n",
            " [0.4804584  0.63174505 0.         0.         0.         0.4804584\n",
            "  0.37311881]]\n"
          ]
        }
      ],
      "source": [
        "#the default of min_df is one\n",
        "doc1 = \"the quick brown fox\"\n",
        "doc2 = \"the lazy dog\"\n",
        "doc3 = \"the quick brown cat\"\n",
        "\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform([doc1, doc2, doc3])\n",
        "print(cv.get_feature_names_out())  # ['brown', 'cat', 'dog', 'fox', 'lazy', 'quick', 'the']\n",
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = TfidfVectorizer(min_df=2)\n",
        "X = cv.fit_transform([doc1, doc2, doc3])\n",
        "print(cv.get_feature_names_out())  # ['brown', 'quick', 'the']\n",
        "print(X.toarray())\n",
        "\n",
        "#noticed thatcat,dog,fox,lazy]>>>>>> not appeared becuse the numbers of repeats less than two"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTGkscNvE4DL",
        "outputId": "dd4dc39c-069a-4fe6-bd75-d2cc60f2b537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown' 'quick' 'the']\n",
            "[[0.61980538 0.61980538 0.48133417]\n",
            " [0.         0.         1.        ]\n",
            " [0.61980538 0.61980538 0.48133417]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = TfidfVectorizer(min_df=2,stop_words='english')\n",
        "X = cv.fit_transform([doc1, doc2, doc3])\n",
        "print(cv.get_feature_names_out())  # ['brown', 'quick', 'the']\n",
        "print(X.toarray())\n",
        "\n",
        "#noticed that [the]>>>>>>>>not appeared becuse it is from stop_word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvtC_M32FZjc",
        "outputId": "f5c84bdc-50c6-401d-feeb-33ca89b8a936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown' 'quick']\n",
            "[[0.70710678 0.70710678]\n",
            " [0.         0.        ]\n",
            " [0.70710678 0.70710678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Another Example"
      ],
      "metadata": {
        "id": "bgqSoNHyKjJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=[\"Hi How are you How are you doing\",\"Hi what's up\",\"Wow that's awesome\"]\n",
        "cv = TfidfVectorizer(min_df=1)\n",
        "x_traincv = cv.fit_transform(text)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "# Print the feature names and the TF-IDF matrix\n",
        "print(feature_names)\n",
        "print(x_traincv.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScLzGnNZIN7I",
        "outputId": "0b80a644-b5b2-4dac-8ae5-a117b7cd5eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'awesome' 'doing' 'hi' 'how' 'that' 'up' 'what' 'wow' 'you']\n",
            "[[0.54275734 0.         0.27137867 0.20639047 0.54275734 0.\n",
            "  0.         0.         0.         0.54275734]\n",
            " [0.         0.         0.         0.4736296  0.         0.\n",
            "  0.62276601 0.62276601 0.         0.        ]\n",
            " [0.         0.57735027 0.         0.         0.         0.57735027\n",
            "  0.         0.         0.57735027 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Another Example\n",
        "\n",
        "text=[\"Hi How are you How are you doing\",\"Hi what's up\",\"Wow that's awesome\"]\n",
        "cv = TfidfVectorizer(min_df=1,stop_words='english')\n",
        "x_traincv = cv.fit_transform(text)\n",
        "feature_names = cv.get_feature_names_out()\n",
        "\n",
        "# Print the feature names and the TF-IDF matrix\n",
        "print(feature_names)\n",
        "print(x_traincv.toarray())"
      ],
      "metadata": {
        "id": "SuzvrXTaKWOO",
        "outputId": "43287885-ce64-429c-fe94-297fd318d033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['awesome' 'doing' 'hi' 'wow']\n",
            "[[0.         0.79596054 0.60534851 0.        ]\n",
            " [0.         0.         1.         0.        ]\n",
            " [0.70710678 0.         0.         0.70710678]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word Embedding"
      ],
      "metadata": {
        "id": "ccr--90dzaHB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyYwWTSZJH5W"
      },
      "source": [
        "### Word Embedding Techniques using Embedding Layer in Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al-PwLlTJH5Z"
      },
      "outputs": [],
      "source": [
        "### Libraries USed Tensorflow> 2.0  and keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNfAbxF_JH5a"
      },
      "outputs": [],
      "source": [
        "##tensorflow >2.0\n",
        "from tensorflow.keras.preprocessing.text import one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN8Qn9u-JH5a"
      },
      "outputs": [],
      "source": [
        "### sentences\n",
        "sent=[  'the glass of milk',\n",
        "     'the glass of juice',\n",
        "     'the cup of tea',\n",
        "    'I am a good boy',\n",
        "     'I am a good developer',\n",
        "     'understand the meaning of words',\n",
        "     'your videos are good',]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGmGq2G4JH5b",
        "outputId": "c8299b2c-71a6-4301-e70f-b1537d70b028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the glass of milk',\n",
              " 'the glass of juice',\n",
              " 'the cup of tea',\n",
              " 'I am a good boy',\n",
              " 'I am a good developer',\n",
              " 'understand the meaning of words',\n",
              " 'your videos are good']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "sent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1-  In general, a larger vocabulary size allows for a more nuanced representation of the language, but it also increases the computational cost of training the embedding model. On the other hand, a smaller vocabulary size may limit the expressiveness of the embeddings and reduce the accuracy of the trained model.\n",
        "\n",
        "2- A reasonable starting point for a vocabulary size is around 20,000 to 30,000, but this may need to be adjusted up or down depending on your specific use case. If you have a smaller dataset or are working with a less complex language, you may be able to use a smaller vocabulary size"
      ],
      "metadata": {
        "id": "sEPuoylJqKTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxPwKSsfJH5c"
      },
      "outputs": [],
      "source": [
        "### Vocabulary size>>>> the size of one hot encoding\n",
        "voc_size=10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baaVqWn4JH5c"
      },
      "source": [
        "#### One Hot Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc4ikAQiJH5c",
        "outputId": "cba3a958-1af2-44da-fdcd-800ad92c7c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2725, 4751, 1322, 7778], [2725, 4751, 1322, 8804], [2725, 4234, 1322, 5633], [1577, 8806, 7300, 4252, 1890], [1577, 8806, 7300, 4252, 900], [5473, 2725, 6925, 1322, 1240], [4436, 7782, 7303, 4252]]\n"
          ]
        }
      ],
      "source": [
        "#will make one hot encoding and return each sentence with it's indices\n",
        "\n",
        "onehot_repr=[one_hot(words,voc_size)for words in sent]\n",
        "print(onehot_repr)\n",
        "\n",
        "\n",
        "#[2725, 4751, 1322, 7778] >>>>>the glass of milk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtCaZYeTJH5d"
      },
      "source": [
        "### Word Embedding Represntation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-8SZmDnJH5d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- here, we will pass the result of one hot to embedding layer.\n",
        "\n",
        "2- but not all the sentences have same numbers of words , so i will make padding.\n",
        "\n",
        "3- padding='pre' or 'post'>>>> the zeros before or after indices to be same size.\n",
        "\n",
        "4- The choice of padding can affect the performance of the model, depending on the nature of the data. For example, if the end of the sequence is more important for the task at hand, you may want to use padding='pre' to preserve the order of the original sequence. On the other hand, if the beginning of the sequence is more important, you may want to use padding='post'. In some cases, it may be useful to try both options and see which one works better for your particular task."
      ],
      "metadata": {
        "id": "55CUO8a1tQNJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUBNvvjEJH5d",
        "outputId": "7ff90a81-530b-4256-cd4a-d6243813cac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0    0 2725 4751 1322 7778]\n",
            " [   0    0    0    0 2725 4751 1322 8804]\n",
            " [   0    0    0    0 2725 4234 1322 5633]\n",
            " [   0    0    0 1577 8806 7300 4252 1890]\n",
            " [   0    0    0 1577 8806 7300 4252  900]\n",
            " [   0    0    0 5473 2725 6925 1322 1240]\n",
            " [   0    0    0    0 4436 7782 7303 4252]]\n"
          ]
        }
      ],
      "source": [
        "sent_length=8\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJiH4HWJJH5e"
      },
      "outputs": [],
      "source": [
        "dim=10\n",
        "\n",
        "# tis is the attributes which will compare the words with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM2mOZAMJH5e"
      },
      "outputs": [],
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(voc_size,10,input_length=sent_length))\n",
        "model.compile('adam','mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzE_QuufJH5e",
        "outputId": "b1f3e697-588d-4eb5-cbc8-d75bea1f4a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 8, 10)             100000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 100,000\n",
            "Trainable params: 100,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePJNKiNwJH5e",
        "outputId": "2a05e8c9-5048-40d7-fd55-9e9dd8bc8b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 49ms/step\n",
            "[[[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862\n",
            "    0.04609648  0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            "  [ 0.04637647  0.04674262  0.02558163  0.02132613 -0.00860794\n",
            "    0.02643653  0.02292861  0.04576779  0.04208876  0.03885684]\n",
            "  [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364\n",
            "   -0.02045926  0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            "  [-0.04474065 -0.00670535 -0.01946297 -0.04355031  0.0459247\n",
            "    0.01831294  0.02167111  0.00170071  0.04803351 -0.00587826]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862\n",
            "    0.04609648  0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            "  [ 0.04637647  0.04674262  0.02558163  0.02132613 -0.00860794\n",
            "    0.02643653  0.02292861  0.04576779  0.04208876  0.03885684]\n",
            "  [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364\n",
            "   -0.02045926  0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            "  [-0.02535597  0.04952774  0.02960017 -0.04968152  0.04142809\n",
            "    0.03051211  0.015211   -0.02031036 -0.02720711 -0.04824958]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862\n",
            "    0.04609648  0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            "  [ 0.01748414  0.02312697 -0.03764137  0.01068668  0.04101438\n",
            "   -0.03984879  0.00070301 -0.01221848 -0.03392565 -0.04473927]\n",
            "  [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364\n",
            "   -0.02045926  0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            "  [-0.00509753 -0.04518769  0.03374041 -0.03283124  0.00981448\n",
            "   -0.03343339 -0.01480009  0.04185853  0.03269669  0.01858939]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [ 0.04487356  0.00184439 -0.02532619 -0.03945268 -0.02196453\n",
            "    0.04463129 -0.04043924 -0.00716727  0.01363024  0.00027088]\n",
            "  [ 0.03633343 -0.04277376 -0.01806928 -0.02134807 -0.04304317\n",
            "   -0.0254111   0.02762497 -0.02956953  0.01324072 -0.00037248]\n",
            "  [ 0.01151053  0.00514559  0.01138214  0.00996524 -0.03470706\n",
            "   -0.02313011 -0.01871405  0.03544158  0.01862435 -0.01940693]\n",
            "  [-0.00466949 -0.00527953  0.04243873 -0.00255273  0.03985054\n",
            "   -0.02516247  0.03026782 -0.02143025 -0.03775815  0.01316437]\n",
            "  [-0.04871919  0.02635391 -0.00201228  0.02078035 -0.03268728\n",
            "   -0.04459599  0.03166293  0.00395114  0.00532113 -0.02726661]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [ 0.04487356  0.00184439 -0.02532619 -0.03945268 -0.02196453\n",
            "    0.04463129 -0.04043924 -0.00716727  0.01363024  0.00027088]\n",
            "  [ 0.03633343 -0.04277376 -0.01806928 -0.02134807 -0.04304317\n",
            "   -0.0254111   0.02762497 -0.02956953  0.01324072 -0.00037248]\n",
            "  [ 0.01151053  0.00514559  0.01138214  0.00996524 -0.03470706\n",
            "   -0.02313011 -0.01871405  0.03544158  0.01862435 -0.01940693]\n",
            "  [-0.00466949 -0.00527953  0.04243873 -0.00255273  0.03985054\n",
            "   -0.02516247  0.03026782 -0.02143025 -0.03775815  0.01316437]\n",
            "  [ 0.04865993 -0.03456859 -0.02072412  0.03160434 -0.0299446\n",
            "    0.02177337 -0.04458305  0.02414345  0.02352862 -0.04343603]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00445364 -0.03477461  0.03108548  0.00035025  0.03509876\n",
            "    0.02532608 -0.04478619 -0.03337078 -0.02820891 -0.01110876]\n",
            "  [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862\n",
            "    0.04609648  0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            "  [-0.03176175  0.03646977 -0.01935636  0.02355638 -0.04079852\n",
            "    0.02276872  0.03518913 -0.02455413 -0.04791397 -0.0350433 ]\n",
            "  [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364\n",
            "   -0.02045926  0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            "  [-0.04963126  0.02281773 -0.02396283  0.00921614 -0.00544395\n",
            "   -0.02191246 -0.01802733  0.02314908  0.03828922 -0.00433064]]\n",
            "\n",
            " [[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743\n",
            "   -0.02600037  0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            "  [ 0.04496891 -0.01567414 -0.04688671  0.01868448 -0.0157622\n",
            "    0.02454749  0.04125459 -0.01539049 -0.04923397 -0.0095744 ]\n",
            "  [-0.0185936  -0.00405601  0.04090358  0.00277416  0.03313804\n",
            "   -0.03893951 -0.01439025  0.01004219  0.04022593  0.01793728]\n",
            "  [ 0.00718136  0.02297634 -0.01454089  0.02514254  0.01795458\n",
            "    0.04865357  0.01856035 -0.01496939 -0.03471389  0.0106904 ]\n",
            "  [-0.00466949 -0.00527953  0.04243873 -0.00255273  0.03985054\n",
            "   -0.02516247  0.03026782 -0.02143025 -0.03775815  0.01316437]]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(embedded_docs))\n",
        "#there are the features would be used in the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sraokIRpJH5f",
        "outputId": "ef96750d-3f4c-4cdd-d761-1d817c8d3870"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0, 2725, 4751, 1322, 7778], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#frist sentence after onehot + padding\n",
        "embedded_docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9waC6dFJH5f",
        "outputId": "92f8af33-6282-4b10-943b-01da0cabbc39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "[[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862  0.04609648\n",
            "   0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            " [ 0.04637647  0.04674262  0.02558163  0.02132613 -0.00860794  0.02643653\n",
            "   0.02292861  0.04576779  0.04208876  0.03885684]\n",
            " [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364  -0.02045926\n",
            "   0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            " [-0.04474065 -0.00670535 -0.01946297 -0.04355031  0.0459247   0.01831294\n",
            "   0.02167111  0.00170071  0.04803351 -0.00587826]]\n"
          ]
        }
      ],
      "source": [
        "#frist sentence after embedding\n",
        "print(model.predict(embedded_docs)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqlU6dM1JH5f",
        "outputId": "a0992c84-eb21-4c30-f5d8-de147437efa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n",
            "[[-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.00920211 -0.00609029 -0.04967015 -0.00404746  0.0109743  -0.02600037\n",
            "   0.04772934 -0.03560144 -0.03376504 -0.00454193]\n",
            " [-0.03650075  0.03408836 -0.04835917 -0.03642067  0.03801862  0.04609648\n",
            "   0.04647345 -0.04785673 -0.00879307  0.00588372]\n",
            " [ 0.04637647  0.04674262  0.02558163  0.02132613 -0.00860794  0.02643653\n",
            "   0.02292861  0.04576779  0.04208876  0.03885684]\n",
            " [ 0.00605949  0.0075675   0.04031825 -0.00727053 -0.0169364  -0.02045926\n",
            "   0.00146352  0.03443665 -0.04293823  0.04288236]\n",
            " [-0.02535597  0.04952774  0.02960017 -0.04968152  0.04142809  0.03051211\n",
            "   0.015211   -0.02031036 -0.02720711 -0.04824958]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict(embedded_docs)[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DDbDAHMjKg6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CtJ2nVPqluq2",
        "JlNxwU4emHQs",
        "v2G9S9SamtU5",
        "CTDrLBHjpn8s",
        "wbMYT37ht1d7",
        "p5RAWJj_9wQh",
        "Jx9X26LW-yuy",
        "IT478mmsw6nI",
        "UUrhx_p53hrK",
        "YnzqQ88W4SI_",
        "2xKEtJzaN3RQ",
        "hBgOLzIrSH5O",
        "iMCQUGPRSsZG",
        "bu6v1JbMS_12",
        "RjGt0FRfi8w1",
        "oFec2wMNoy16",
        "lXZ3x4YB1mTP",
        "mdGK-oL2bS5B",
        "YxU5AJljkEis",
        "fja1Hz8xkLCf",
        "iT_sLD4lkvTY",
        "tbmAW3Mo2LDh",
        "KyYwWTSZJH5W",
        "WtCaZYeTJH5d"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}